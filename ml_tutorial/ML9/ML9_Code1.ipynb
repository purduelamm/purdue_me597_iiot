{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCtYNxU1obTp"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDRyFk_aogGp"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUNz_xYMmjm5"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRFcx-HclW7t"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ie0qFvRImqWJ"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9165,
     "status": "ok",
     "timestamp": 1740955850061,
     "user": {
      "displayName": "HOJUN LEE",
      "userId": "02533503466870413951"
     },
     "user_tz": 300
    },
    "id": "oadW8SNVmuJc",
    "outputId": "a23bc3a6-b967-4771-b1df-4391919f254e"
   },
   "outputs": [],
   "source": [
    "# Hyperparameter related to loading our dataset\n",
    "IMG_SIZE = (160, 160)\n",
    "\n",
    "# We are going to download open-sourced dataset to demonstrate transfer learning\n",
    "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
    "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True, cache_subdir='/content/')\n",
    "PATH = f'{path_to_zip}/cats_and_dogs_filtered'\n",
    "\n",
    "# Generate tf.data.Datset for training and validation dataset\n",
    "# This allows us to efficiently load batch of data to our NN model for training and validation\n",
    "# FYI: https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "train_dir = os.path.join(PATH, 'train')\n",
    "validation_dir = os.path.join(PATH, 'validation')\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir, shuffle=True, image_size=IMG_SIZE)\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir, shuffle=True, image_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMsHdf_SqI-5"
   },
   "source": [
    "## Dataset Splitting\n",
    "\n",
    "Conventionally, researchers and engineers split the entire dataset into three groups: 1) Train dataset, 2) Validation dataset, 3) Test dataset. Typically, 1) is directly utilized to train our NN while 2) is used to log how the model learns during the training. On the other hand, 3) is literally used to test how well the model generalizes across new dataset, which is an important performance metric of NN models.\n",
    "\n",
    "**Now, the question is: why are we introducing 2) during the training if it doesn't even used for the actual training process?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1740955850068,
     "user": {
      "displayName": "HOJUN LEE",
      "userId": "02533503466870413951"
     },
     "user_tz": 300
    },
    "id": "zvPS6WJ6qIJO",
    "outputId": "703d982e-4ba5-49e8-ce79-e9974e08ac24"
   },
   "outputs": [],
   "source": [
    "# The original dataset doesn't contain a test set.\n",
    "# So, we are going to create one. To do so, we determine how many batches of data are available in the validation set using tf.data.experimental.cardinality.\n",
    "# We will use 20% of the validation data as a test set.\n",
    "\n",
    "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
    "test_dataset = validation_dataset.take(val_batches // 5)\n",
    "validation_dataset = validation_dataset.skip(val_batches // 5)\n",
    "\n",
    "print(f'# of validation batches: {tf.data.experimental.cardinality(validation_dataset)}')\n",
    "print(f'# of test batches: {tf.data.experimental.cardinality(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS__hK0BCKqZ"
   },
   "source": [
    "## Data Augmentation\n",
    "\n",
    "As there are usually a lot of parameters to tune during the training phase, diversity of the data with in a dataset is a huge matter to build a high performance model (i.e., avoiding overfitting). To do so, researchers have proposed various technniques in many different levels of model training.\n",
    "\n",
    "In particular, **data augmentation** is a technique where you increases vaariability of the data within the dataset **by applying various filters to obtain different views of the original data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMnCmpTVCM6G"
   },
   "outputs": [],
   "source": [
    "# We are going to use very basic image augmentation to increase variability of our dataset.\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip('horizontal'),\n",
    "  tf.keras.layers.RandomRotation(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCSp281MphhG"
   },
   "source": [
    "## Data Inspection & Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "executionInfo": {
     "elapsed": 4062,
     "status": "ok",
     "timestamp": 1740955854140,
     "user": {
      "displayName": "HOJUN LEE",
      "userId": "02533503466870413951"
     },
     "user_tz": 300
    },
    "id": "ZqCFs0DJpwAd",
    "outputId": "eaeb6c8e-1a4b-4b63-fc62-ab1105266b6b"
   },
   "outputs": [],
   "source": [
    "# Let's visualize several images in our dataset\n",
    "class_names = train_dataset.class_names\n",
    "\n",
    "for image, _ in train_dataset.take(1):\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  first_image = image[0]\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "    plt.imshow(augmented_image[0] / 255)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkiPbF75uRdx"
   },
   "source": [
    "## Rescale Pixel Values\n",
    "\n",
    "In a moment, we will download `tf.keras.applications.MobileNetV2` for use as your base model. This model expects pixel values in `[-1, 1]`, but at this point, the pixel values in your images are in `[0, 255]`. Hence, we need to adjust this for training our own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zkoB9rDhunDj"
   },
   "outputs": [],
   "source": [
    "# To rescale them, we use the preprocessing method included with the model.\n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XY-ymP2uvlI"
   },
   "source": [
    "## Base Model (i.e., Backbone) Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1711,
     "status": "ok",
     "timestamp": 1740955855856,
     "user": {
      "displayName": "HOJUN LEE",
      "userId": "02533503466870413951"
     },
     "user_tz": 300
    },
    "id": "ddggCilLu0dV",
    "outputId": "0287923f-159d-459e-9b64-41641d9a18e0"
   },
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "IMG_SHAPE = IMG_SIZE + (3,) # (160, 160, 3) -> RGB image\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=, weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbColBRXvNtB"
   },
   "source": [
    "## Method 2. Fine Tuning\n",
    "\n",
    "Fine Tuning is a method where you integrate a base model (i.e., **backbone**) with your own **head**. This method requires a bit more computational effort to build your model as weights and biases of both the base model and your own head should be updated during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1267,
     "status": "ok",
     "timestamp": 1740955857120,
     "user": {
      "displayName": "HOJUN LEE",
      "userId": "02533503466870413951"
     },
     "user_tz": 300
    },
    "id": "oICuzdSuvNWV",
    "outputId": "34dbe901-f3f0-49f2-a009-bfe556c39f12"
   },
   "outputs": [],
   "source": [
    "# This will \"unfreeze\" weights and biases in our base model.\n",
    "# In other words, there will be no backpropagation of the loss graidents to update weights and biases in the NN.\n",
    "base_model.trainable =\n",
    "\n",
    "# Now, let's design our own classification head for our own application\n",
    "# When generating your own NN model, make sure that intermediate dimenions between layers are correct!\n",
    "# Let's first check output dimension of the base_model\n",
    "image_batch, label_batch = next(iter(train_dataset))\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)\n",
    "\n",
    "# Since we are going to design a simple binary classifiation model, we will use average pooling layer to obtain a global feature of an inpute image.\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)\n",
    "\n",
    "# Then, make sure that we get the expected shape after going to our classification head.\n",
    "prediction_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)\n",
    "\n",
    "# Finally, let's build our model.\n",
    "# We will also check whether our model works as expected. We will use a dummy image to test the model.\n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = # Add data augmentation\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "\n",
    "# Add dropout layer\n",
    "x = # This may cause some performance gap between training and validation\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 100\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 485,
     "status": "ok",
     "timestamp": 1740955857606,
     "user": {
      "displayName": "HOJUN LEE",
      "userId": "02533503466870413951"
     },
     "user_tz": 300
    },
    "id": "7XeOpn3OB--d",
    "outputId": "9553a3cd-0f25-435b-e616-1c2ec01e1b5e"
   },
   "outputs": [],
   "source": [
    "# Check how many parameters are trainable\n",
    "model.summary()\n",
    "len(model.trainable_variables)\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9CTFzOkxJrZ"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 121592,
     "status": "ok",
     "timestamp": 1740955979200,
     "user": {
      "displayName": "HOJUN LEE",
      "userId": "02533503466870413951"
     },
     "user_tz": 300
    },
    "id": "ijrNc3NPxVTY",
    "outputId": "ab22e5bf-eccb-4937-a670-8cdc3df5f6f9"
   },
   "outputs": [],
   "source": [
    "base_learning_rate = # Another important hyperparameter to train our model\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10), # Note that the learning rate is much lower.\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5, name='accuracy')])\n",
    "\n",
    "\n",
    "# Let's see how this process will improve our NN after 20 epochs of training.\n",
    "epochs = # We have now more epochs to train.\n",
    "loss0, accuracy0 = model.evaluate(validation_dataset)\n",
    "\n",
    "print(f\"initial loss: {loss0:.2f}\")\n",
    "print(f\"initial accuracy: {accuracy0:.2f}\")\n",
    "\n",
    "history = model.fit(train_dataset, epochs=epochs, validation_data=validation_dataset)\n",
    "\n",
    "# Visualization of loss and accuracy during the training process\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 933
    },
    "executionInfo": {
     "elapsed": 3438,
     "status": "ok",
     "timestamp": 1740955982643,
     "user": {
      "displayName": "HOJUN LEE",
      "userId": "02533503466870413951"
     },
     "user_tz": 300
    },
    "id": "m7Yemuf_BcOj",
    "outputId": "2bd7bcaa-e455-4312-f87a-75384dd235a3"
   },
   "outputs": [],
   "source": [
    "# Now, let's check how the model works on the test dataset\n",
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print('Test accuracy :', accuracy)\n",
    "\n",
    "# Retrieve a batch of images from the test set\n",
    "image_batch, label_batch = test_dataset.as_numpy_iterator().next()\n",
    "predictions = model.predict_on_batch(image_batch).flatten()\n",
    "predictions = tf.where(predictions < 0.5, 0, 1)\n",
    "\n",
    "print('Predictions:\\n', predictions.numpy())\n",
    "print('Labels:\\n', label_batch)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "  ax = plt.subplot(3, 3, i + 1)\n",
    "  plt.imshow(image_batch[i].astype(\"uint8\"))\n",
    "  plt.title(class_names[predictions[i]])\n",
    "  plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPSrQ5VEL8VSVJMyAdjimLJ",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
